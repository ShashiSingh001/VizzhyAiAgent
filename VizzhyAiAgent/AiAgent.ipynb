{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from urllib.parse import quote, urljoin\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openpyxl  # Ensure Excel compatibility\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        return \"\\n\".join([page.get_text(\"text\") for page in doc]).strip() or None\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Error processing PDF {pdf_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_text_from_csv(csv_path):\n",
    "    \"\"\"Extract text from a CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, dtype=str, encoding=\"utf-8-sig\", encoding_errors=\"replace\")\n",
    "        return \"\\n\".join(df.apply(lambda x: ' '.join(x.dropna()), axis=1)).strip() or None\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Error processing CSV {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_text_from_excel(excel_path):\n",
    "    \"\"\"Extract text from an Excel file, handling multiple sheets.\"\"\"\n",
    "    try:\n",
    "        sheets = pd.read_excel(excel_path, sheet_name=None, dtype=str, engine=\"openpyxl\")\n",
    "        text = \"\\n\".join(\"\\n\".join(df.apply(lambda x: ' '.join(x.dropna()), axis=1)) for df in sheets.values()).strip()\n",
    "        return text or None\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Error processing Excel {excel_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=500):\n",
    "    \"\"\"Split text into smaller chunks.\"\"\"\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i: i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "def generate_dataset_link(file_name, file_path, use_cloud=False):\n",
    "    \"\"\"Generate a dataset link for local or cloud storage.\"\"\"\n",
    "    if use_cloud:\n",
    "        return urljoin(\"https://your-cloud-storage.com/datasets/\", quote(file_name))\n",
    "    return f\"file:///{quote(os.path.abspath(file_path).replace('\\\\', '/'))}\"\n",
    "\n",
    "def process_files(data_dir):\n",
    "    \"\"\"Process all supported files in the specified directory.\"\"\"\n",
    "    if not os.path.isdir(data_dir):\n",
    "        raise FileNotFoundError(f\"âŒ Data directory not found: {data_dir}\")\n",
    "    \n",
    "    all_files = os.listdir(data_dir)\n",
    "    print(f\"ðŸ“‚ Found {len(all_files)} files in data directory.\")\n",
    "    structured_data = []\n",
    "    \n",
    "    for file in all_files:\n",
    "        file_path = os.path.join(data_dir, file)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"âš  File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        text, file_type = None, None\n",
    "        \n",
    "        if file.lower().endswith(\".pdf\"):\n",
    "            text, file_type = extract_text_from_pdf(file_path), \"PDF\"\n",
    "        elif file.lower().endswith(\".csv\"):\n",
    "            text, file_type = extract_text_from_csv(file_path), \"CSV\"\n",
    "        elif file.lower().endswith(\".xlsx\"):\n",
    "            text, file_type = extract_text_from_excel(file_path), \"Excel\"\n",
    "        else:\n",
    "            print(f\"âš  Skipping unsupported file type: {file}\")\n",
    "            continue\n",
    "        \n",
    "        if not text:\n",
    "            print(f\"âš  Skipping empty/unreadable file: {file}\")\n",
    "            continue\n",
    "        \n",
    "        chunks = split_text_into_chunks(text, chunk_size=500)\n",
    "        dataset_link = generate_dataset_link(file, file_path, use_cloud=False)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            structured_data.append({\n",
    "                \"file_name\": file,\n",
    "                \"file_type\": file_type,\n",
    "                \"file_link\": dataset_link,\n",
    "                \"chunk_index\": i,\n",
    "                \"text_chunk\": chunk\n",
    "            })\n",
    "    \n",
    "    if not structured_data:\n",
    "        raise ValueError(\"âš  No valid text chunks found! Ensure files contain readable text.\")\n",
    "    \n",
    "    print(f\"âœ… Processed {len(set(d['file_name'] for d in structured_data))} files and stored {len(structured_data)} text chunks.\")\n",
    "    \n",
    "    with open(\"structured_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(structured_data, json_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    return structured_data\n",
    "\n",
    "def compute_embeddings(structured_data):\n",
    "    \"\"\"Compute text embeddings and store them in a FAISS index.\"\"\"\n",
    "    batch_size = 32\n",
    "    all_embeddings = []\n",
    "    num_chunks = len(structured_data)\n",
    "    \n",
    "    for i in range(0, num_chunks, batch_size):\n",
    "        batch = [item[\"text_chunk\"] for item in structured_data[i: i + batch_size]]\n",
    "        batch_embeddings = embedding_model.encode(batch, convert_to_tensor=True).cpu().numpy()\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    embeddings = np.vstack(all_embeddings).astype(\"float32\")\n",
    "    \n",
    "    if embeddings.shape[0] > 0:\n",
    "        index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "        index.add(embeddings)\n",
    "        faiss.write_index(index, \"vector_database.index\")\n",
    "        print(f\"âœ… Successfully stored {num_chunks} text chunks!\")\n",
    "    else:\n",
    "        raise ValueError(\"âš  No embeddings found! Ensure files contain readable text.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_directory = r\"C:\\Users\\My PC\\Downloads\\VizzhyAiAgent\\VizzhyAiAgent\\data\"\n",
    "    processed_data = process_files(data_directory)\n",
    "    compute_embeddings(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, render_template, send_file\n",
    "import faiss\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import threading\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from flask_cors import CORS\n",
    "import logging\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# Enable logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define constants\n",
    "REQUIRED_FILES = [\"vector_database.index\", \"structured_data.json\"]\n",
    "RELEVANCE_THRESHOLD = 1.5\n",
    "TOP_K_RESULTS = 3\n",
    "AI_API_KEY = \"akm5535x-m60d44cc-39dbca60-fccb8d59\"\n",
    "AI_ENDPOINT_URL = \"https://api.us.inc/hanooman/router/v1/chat/completions\"\n",
    "USER_QUERY_HISTORY = {}\n",
    "QUERY_LOCK = threading.Lock()\n",
    "DATA_DIR = r\"C:\\Users\\My PC\\Downloads\\VizzhyAiAgent\\VizzhyAiAgent\\data\"\n",
    "\n",
    "# Verify required files exist\n",
    "for file in REQUIRED_FILES:\n",
    "    if not os.path.exists(file):\n",
    "        logging.error(f\"Required file missing: {file}\")\n",
    "        raise FileNotFoundError(f\"Required file missing: {file}\")\n",
    "\n",
    "# Load FAISS index\n",
    "try:\n",
    "    index = faiss.read_index(\"vector_database.index\")\n",
    "    logging.info(\"FAISS index loaded successfully!\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading FAISS index: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# Load structured data\n",
    "try:\n",
    "    with open(\"structured_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        structured_data = json.load(f)\n",
    "    logging.info(f\"Loaded {len(structured_data)} structured text chunks!\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading structured data: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# Load SentenceTransformer model\n",
    "try:\n",
    "    embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    logging.info(\"Embedding model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading embedding model: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "def search_faiss(query, top_k=TOP_K_RESULTS):\n",
    "    \"\"\"Search FAISS index for relevant text chunks.\"\"\"\n",
    "    try:\n",
    "        query_vector = embedding_model.encode(query, convert_to_tensor=True).cpu().numpy().reshape(1, -1)\n",
    "        distances, indices = index.search(query_vector, top_k)\n",
    "        \n",
    "        valid_indices = [idx for idx in indices[0] if 0 <= idx < len(structured_data)]\n",
    "        retrieved_chunks = [structured_data[idx] for idx in valid_indices]\n",
    "        \n",
    "        if not retrieved_chunks or distances[0][0] > RELEVANCE_THRESHOLD:\n",
    "            return \"Out of context\", []\n",
    "        \n",
    "        return \"Relevant\", retrieved_chunks\n",
    "    except Exception as e:\n",
    "        logging.error(f\"FAISS search error: {str(e)}\")\n",
    "        return \"Error\", []\n",
    "\n",
    "\n",
    "def query_ai(query, retrieved_chunks, user_id, expected_items=20):\n",
    "    \"\"\"Query AI model with retrieved context.\"\"\"\n",
    "    context = \"\\n\".join(chunk.get(\"text_chunk\", \"\") for chunk in retrieved_chunks)\n",
    "    user_history = \"\\n\".join(USER_QUERY_HISTORY.get(user_id, []))\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"everest\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a chatbot providing responses based ONLY on the given context. Ensure your response contains exactly {expected_items} complete items if requested.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"User History:\\n{user_history}\\n\\nQuery: {query}\\n\\nGIVEN CONTEXT:\\n{context}\"}\n",
    "        ],\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 1024\n",
    "    }\n",
    "    headers = {\"Authorization\": f\"Bearer {AI_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(AI_ENDPOINT_URL, headers=headers, json=payload, timeout=90)\n",
    "        response.raise_for_status()\n",
    "        ai_response = response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"Error: No valid response.\")\n",
    "        \n",
    "        # Ensure the response contains expected number of items\n",
    "        if \"genes\" in query.lower() and len(re.findall(r\"\\d+\\. \", ai_response)) < expected_items:\n",
    "            logging.warning(\"AI response is incomplete. Retrying with strict constraints.\")\n",
    "            return query_ai(query, retrieved_chunks, user_id, expected_items)\n",
    "        \n",
    "        return ai_response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"AI service error: {str(e)}\")\n",
    "        return \"Error: AI service is unavailable.\"\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"Vizzhy.html\")\n",
    "\n",
    "\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "    data = request.json\n",
    "    user_message = data.get(\"message\", \"\").strip()\n",
    "    user_id = data.get(\"user_id\", \"default_user\")\n",
    "    \n",
    "    if not user_message:\n",
    "        return jsonify({\"reply\": \"Please provide a question or message.\"})\n",
    "    \n",
    "    with QUERY_LOCK:\n",
    "        USER_QUERY_HISTORY.setdefault(user_id, []).append(user_message)\n",
    "        if len(USER_QUERY_HISTORY[user_id]) > 10:\n",
    "            USER_QUERY_HISTORY[user_id] = USER_QUERY_HISTORY[user_id][-10:]\n",
    "    \n",
    "    status, retrieved_chunks = search_faiss(user_message)\n",
    "    if status == \"Out of context\":\n",
    "        return jsonify({\"reply\": \"I cannot answer that based on the available data.\"})\n",
    "    \n",
    "    ai_response = query_ai(user_message, retrieved_chunks, user_id, expected_items=20)\n",
    "    return jsonify({\"reply\": ai_response})\n",
    "\n",
    "\n",
    "@app.route(\"/download/<path:filename>\")\n",
    "def download_file(filename):\n",
    "    file_path = os.path.join(DATA_DIR, filename)\n",
    "    return send_file(file_path, as_attachment=True) if os.path.exists(file_path) else jsonify({\"error\": \"File not found!\"}), 404\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        app.run(host=\"0.0.0.0\", port=5000, debug=False)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Flask app failed to start: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
